"""
Adaptive Knowledge Base Analyzer - è‡ªé€‚åº”çŸ¥è¯†åº“åˆ†æžå™¨
Automatically analyzes knowledge base content and builds dynamic retrieval indices.

Features:
- Automatic term extraction from knowledge base documents
- Dynamic keyword-to-document mapping
- Periodic index rebuilding
- Content-aware retrieval enhancement
"""

import logging
import re
import json
from typing import Dict, List, Any, Optional, Set, Tuple
from collections import defaultdict
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, update
from app.models.models import KnowledgeBaseChunk
import jieba
import jieba.posseg as pseg

logger = logging.getLogger(__name__)


class KnowledgeBaseAnalyzer:
    """
    Analyzes knowledge base content and builds dynamic indices
    """
    
    def __init__(self, db: AsyncSession):
        self.db = db
        self._term_index: Dict[str, List[Dict]] = defaultdict(list)
        self._document_terms: Dict[str, Set[str]] = {}
        self._medical_vocab: Set[str] = set()
    
    async def build_indices(self) -> Dict[str, Any]:
        """
        Build dynamic indices from all knowledge base content
        """
        logger.info("ðŸ—ï¸ Building adaptive knowledge base indices...")
        
        # Get all active chunks
        stmt = select(KnowledgeBaseChunk).where(KnowledgeBaseChunk.is_active == True)
        result = await self.db.execute(stmt)
        chunks = result.scalars().all()
        
        logger.info(f"Analyzing {len(chunks)} knowledge base chunks...")
        
        # Clear existing indices
        self._term_index.clear()
        self._document_terms.clear()
        self._medical_vocab.clear()
        
        # Analyze each chunk
        for chunk in chunks:
            await self._analyze_chunk(chunk)
        
        # Build statistics
        stats = {
            'total_chunks': len(chunks),
            'unique_terms': len(self._term_index),
            'medical_terms': len(self._medical_vocab),
            'top_terms': self._get_top_terms(20)
        }
        
        logger.info(f"âœ… Index building complete: {stats['unique_terms']} terms, "
                   f"{stats['medical_terms']} medical terms")
        
        return stats
    
    async def _analyze_chunk(self, chunk: KnowledgeBaseChunk):
        """
        Analyze a single chunk and extract terms
        """
        text = chunk.chunk_text or ""
        if not text:
            return
        
        # Extract medical terms using multiple methods
        terms = self._extract_medical_terms(text)
        
        # Store term-to-chunk mapping
        chunk_ref = {
            'chunk_id': str(chunk.id),
            'text': text[:200],  # Store preview
            'section_title': chunk.section_title,
            'document_title': chunk.document_title,
            'disease_category': chunk.disease_category,
            'similarity_boost': 0
        }
        
        for term in terms:
            self._term_index[term].append(chunk_ref)
            self._medical_vocab.add(term)
        
        # Store document-level terms
        doc_key = f"{chunk.document_title}_{chunk.disease_category}"
        if doc_key not in self._document_terms:
            self._document_terms[doc_key] = set()
        self._document_terms[doc_key].update(terms)
    
    def _extract_medical_terms(self, text: str) -> Set[str]:
        """
        Extract medical terms from text using multiple strategies
        """
        terms = set()
        
        # 1. Extract disease names (ç–¾ç—…åç§°æ¨¡å¼)
        disease_patterns = [
            r'([^ï¼Œã€‚\n]{2,20}(?:ç‚Ž|ç—…|ç—‡|å¾|ç™Œ|ç˜¤|æ¯’|èŒ|è™«|ç¼ºä¹|äº¢è¿›|å‡é€€|ç»¼åˆå¾|ç»¼åˆç—‡))',
            r'([^ï¼Œã€‚\n]{2,15}(?:è‚ºç‚Ž|è‚ç‚Ž|è‚¾ç‚Ž|èƒƒç‚Ž|è‚ ç‚Ž|è„‘ç‚Ž|å¿ƒè‚Œç‚Ž|å…³èŠ‚ç‚Ž))',
            r'(æ€¥æ€§|æ…¢æ€§|è½»åº¦|ä¸­åº¦|é‡åº¦|åŽŸå‘|ç»§å‘)?[^ï¼Œã€‚\n]{2,10}(?:æ„ŸæŸ“|å‡ºè¡€|æ “å¡ž|æ¢—æ­»|åæ­»)',
        ]
        
        for pattern in disease_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                term = match.group(0).strip()
                if len(term) >= 2 and len(term) <= 20:
                    terms.add(term)
        
        # 2. Extract symptoms (ç—‡çŠ¶æ¨¡å¼)
        symptom_patterns = [
            r'(å’³å—½|å’³ç—°|å‘çƒ­|å‘çƒ§|èƒ¸ç—›|è…¹ç—›|å¤´ç—›|å¤´æ™•|æ¶å¿ƒ|å‘•å|è…¹æ³»|ä¾¿ç§˜|çš®ç–¹|ç˜™ç—’|å‡ºè¡€|æ°´è‚¿|ä¼‘å…‹)',
            r'([\w\u4e00-\u9fff]{2,8}(?:ç—›|ç—’|èƒ€|éº»|æœ¨|é…¸|æ²‰|æ™•|å’³|å–˜|çƒ§|çƒ­|å¯’|æŠ–))',
        ]
        
        for pattern in symptom_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                terms.add(match.group(0).strip())
        
        # 3. Extract lab/test indicators (æ£€éªŒæŒ‡æ ‡)
        lab_patterns = [
            r'(ç™½ç»†èƒž|çº¢ç»†èƒž|è¡€å°æ¿|è¡€çº¢è›‹ç™½|CRP|PCT|è¡€æ²‰|ALT|AST|è‚Œé…|å°¿ç´ æ°®|è¡€ç³–|è¡€è„‚)',
            r'(IgM|IgG|IgA|æŠ—åŽŸ|æŠ—ä½“|æ ¸é…¸|åŸ¹å…»|è¯æ•)',
            r'(é˜³æ€§|é˜´æ€§|å¼±é˜³æ€§|å¼±é˜´æ€§|æ­£å¸¸|å¼‚å¸¸|å‡é«˜|é™ä½Ž)',
        ]
        
        for pattern in lab_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                terms.add(match.group(0).strip())
        
        # 4. Extract treatment terms (æ²»ç–—ç›¸å…³)
        treatment_patterns = [
            r'(æŠ—ç”Ÿç´ |æŠ—ç—…æ¯’|æ¿€ç´ |å…ç–«æŠ‘åˆ¶å‰‚|åŒ–ç–—|æ”¾ç–—|æ‰‹æœ¯|è¾“æ¶²|å¸æ°§|ç›‘æŠ¤)',
            r'(é’éœ‰ç´ |å¤´å­¢|é˜¿å¥‡|çº¢éœ‰ç´ |ç”²å¼ºé¾™|åœ°å¡žç±³æ¾|å¸ƒåœ°å¥ˆå¾·)',
            r'(æ²»ç–—|ç”¨è¯|å‰‚é‡|ç–—ç¨‹|é™è„‰|å£æœ|çš®ä¸‹|å¸å…¥)',
        ]
        
        for pattern in treatment_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                terms.add(match.group(0).strip())
        
        # 5. Extract pathogens (ç—…åŽŸä½“)
        pathogen_patterns = [
            r'(æ”¯åŽŸä½“|è¡£åŽŸä½“|å†›å›¢èŒ|ç™¾æ—¥å’³|ç»“æ ¸|æµæ„Ÿ|æ–°å† |æ–°å† |RSV|è…ºç—…æ¯’|è‚ é“ç—…æ¯’)',
            r'(ç»†èŒ|ç—…æ¯’|çœŸèŒ|å¯„ç”Ÿè™«|æ”¯åŽŸä½“|è¡£åŽŸä½“|èžºæ—‹ä½“|ç«‹å…‹æ¬¡ä½“)',
        ]
        
        for pattern in pathogen_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                terms.add(match.group(0).strip())
        
        # 6. Use jieba for medical word segmentation (if available)
        try:
            words = pseg.cut(text)
            for word, flag in words:
                # Keep medical-related POS tags
                if flag in ['n', 'nz', 'v', 'vn', 'a', 'an'] and len(word) >= 2:
                    if self._is_medical_term(word):
                        terms.add(word)
        except Exception as e:
            logger.debug(f"Jieba segmentation error: {e}")
        
        return terms
    
    def _is_medical_term(self, word: str) -> bool:
        """
        Check if a word is likely a medical term
        """
        medical_suffixes = [
            'ç‚Ž', 'ç—…', 'ç—‡', 'ç™Œ', 'ç˜¤', 'æ¯’', 'èŒ', 'è™«',
            'ç‚Ž', 'ç‚Ž', 'ç—…', 'ç—‡', 'è¯', 'ç´ ', 'å‰‚'
        ]
        medical_prefixes = [
            'æŠ—', 'è§£', 'æ­¢', 'éº»', 'é•‡', 'æ¶ˆ', 'åŒ–', 'å…'
        ]
        
        return (
            any(word.endswith(suffix) for suffix in medical_suffixes) or
            any(word.startswith(prefix) for prefix in medical_prefixes)
        )
    
    def _get_top_terms(self, n: int = 20) -> List[Tuple[str, int]]:
        """
        Get top N most frequent terms
        """
        term_freq = [(term, len(chunks)) for term, chunks in self._term_index.items()]
        return sorted(term_freq, key=lambda x: x[1], reverse=True)[:n]
    
    def find_related_chunks(self, query: str, document_texts: List[str] = None, top_k: int = 10) -> List[Dict]:
        """
        Find chunks related to query using the dynamic term index
        """
        # Extract terms from query and documents
        query_terms = self._extract_medical_terms(query)
        
        if document_texts:
            for doc_text in document_texts:
                query_terms.update(self._extract_medical_terms(doc_text))
        
        logger.info(f"Query terms extracted: {query_terms}")
        
        # Score chunks based on term overlap
        chunk_scores = defaultdict(lambda: {'score': 0, 'chunk': None, 'matched_terms': []})
        
        for term in query_terms:
            if term in self._term_index:
                for chunk_ref in self._term_index[term]:
                    chunk_id = chunk_ref['chunk_id']
                    chunk_scores[chunk_id]['score'] += 1
                    chunk_scores[chunk_id]['chunk'] = chunk_ref
                    chunk_scores[chunk_id]['matched_terms'].append(term)
        
        # Convert to list and sort by score
        results = []
        for chunk_id, data in chunk_scores.items():
            if data['chunk']:
                chunk_data = data['chunk'].copy()
                chunk_data['term_match_score'] = data['score']
                chunk_data['matched_terms'] = data['matched_terms']
                results.append(chunk_data)
        
        results.sort(key=lambda x: x['term_match_score'], reverse=True)
        return results[:top_k]
    
    def get_term_suggestions(self, partial_term: str) -> List[str]:
        """
        Get term suggestions for autocomplete
        """
        partial_term = partial_term.lower()
        suggestions = []
        
        for term in self._medical_vocab:
            if partial_term in term.lower():
                suggestions.append(term)
        
        return suggestions[:10]
    
    def export_index_stats(self) -> Dict:
        """
        Export index statistics for monitoring
        """
        return {
            'total_terms': len(self._term_index),
            'medical_vocabulary_size': len(self._medical_vocab),
            'document_count': len(self._document_terms),
            'top_diseases': self._get_terms_by_category('disease'),
            'top_symptoms': self._get_terms_by_category('symptom'),
            'top_pathogens': self._get_terms_by_category('pathogen'),
        }
    
    def _get_terms_by_category(self, category: str) -> List[str]:
        """
        Get terms by inferred category
        """
        category_patterns = {
            'disease': ['ç‚Ž', 'ç—…', 'ç—‡', 'ç™Œ', 'ç˜¤'],
            'symptom': ['ç—›', 'ç—’', 'èƒ€', 'éº»', 'æ™•', 'å’³', 'çƒ­'],
            'pathogen': ['èŒ', 'æ¯’', 'è™«', 'æ”¯åŽŸä½“', 'è¡£åŽŸä½“', 'èžºæ—‹ä½“'],
        }
        
        patterns = category_patterns.get(category, [])
        matching_terms = []
        
        for term in self._medical_vocab:
            if any(pattern in term for pattern in patterns):
                matching_terms.append(term)
        
        return matching_terms[:20]


# Global analyzer instance (will be initialized with DB session)
kb_analyzer = None

async def get_kb_analyzer(db: AsyncSession) -> KnowledgeBaseAnalyzer:
    """Get or create knowledge base analyzer"""
    global kb_analyzer
    if kb_analyzer is None:
        kb_analyzer = KnowledgeBaseAnalyzer(db)
        await kb_analyzer.build_indices()
    return kb_analyzer

async def refresh_kb_indices(db: AsyncSession) -> Dict[str, Any]:
    """Force refresh of knowledge base indices"""
    global kb_analyzer
    kb_analyzer = KnowledgeBaseAnalyzer(db)
    return await kb_analyzer.build_indices()
