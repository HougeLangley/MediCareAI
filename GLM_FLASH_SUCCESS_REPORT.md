# MediCare AI - GLM-4.7-Flash æœ¬åœ°éƒ¨ç½²æŒ‡å— / Local Deployment Guide

> **é‡è¦æç¤º | Important Notice:**
> 
> æœ¬æ–‡æ¡£ä¸ºæŒ‡å¯¼æ€§æ–‡æ¡£ï¼Œç”¨æˆ·éœ€è¦æ ¹æ®è‡ªèº«ç¯å¢ƒé…ç½®æœ¬åœ° AI å¤§æ¨¡å‹å’Œ MinerU æœåŠ¡ã€‚
> 
> This is a guidance document. Users need to configure their own local AI models and MinerU services according to their environment.

---

## ğŸ¯ æ¦‚è¿° | Overview

MediCare AI æ”¯æŒæ¥å…¥æœ¬åœ°éƒ¨ç½²çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œ MinerU æ–‡æ¡£å¤„ç†æœåŠ¡ï¼Œç¡®ä¿æ•°æ®éšç§å’Œå®‰å…¨ã€‚

MediCare AI supports integration with locally deployed Large Language Models (LLM) and MinerU document processing services, ensuring data privacy and security.

---

## ğŸ¤– æœ¬åœ° AI å¤§æ¨¡å‹éƒ¨ç½²é€‰é¡¹ | Local AI Model Deployment Options

### 1. Ollamaï¼ˆæ¨èæ–°æ‰‹ï¼‰| Ollama (Recommended for Beginners)

**ç®€ä»‹ | Introduction:**
Ollama æ˜¯ç›®å‰æœ€ç®€å•æ˜“ç”¨çš„æœ¬åœ°å¤§æ¨¡å‹è¿è¡Œå·¥å…·ï¼Œæ”¯æŒ macOSã€Linux å’Œ Windowsã€‚

Ollama is the easiest-to-use local LLM running tool, supporting macOS, Linux, and Windows.

**å®‰è£…æ­¥éª¤ | Installation:**

```bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# æˆ–è€…ä½¿ç”¨ Homebrew (macOS)
brew install ollama

# Windows: ä¸‹è½½å®‰è£…åŒ…ä» https://ollama.com/download
```

**è¿è¡Œ GLM-4.7-Flash | Run GLM-4.7-Flash:**

```bash
# æ‹‰å–æ¨¡å‹
ollama pull unsloth/glm-4.7b

# è¿è¡ŒæœåŠ¡
ollama serve

# é»˜è®¤ç›‘å¬: http://localhost:11434
```

**é…ç½® MediCare AI | Configure MediCare AI:**

åœ¨ `.env` æ–‡ä»¶ä¸­è®¾ç½®ï¼š

```bash
AI_API_KEY=ollama
AI_API_URL=http://localhost:11434/v1/
AI_MODEL_ID=unsloth/glm-4.7b
```

**å‚è€ƒæ–‡æ¡£ | Reference:**
- å®˜ç½‘: https://ollama.com
- GitHub: https://github.com/ollama/ollama
- æ¨¡å‹åº“: https://ollama.com/library

---

### 2. llama.cppï¼ˆæ¨èé«˜çº§ç”¨æˆ·ï¼‰| llama.cpp (Recommended for Advanced Users)

**ç®€ä»‹ | Introduction:**
llama.cpp æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„ LLM æ¨ç†åº“ï¼Œä½¿ç”¨ C/C++ ç¼–å†™ï¼Œæ”¯æŒå¤šç§é‡åŒ–æ ¼å¼ã€‚

llama.cpp is a high-performance LLM inference library written in C/C++, supporting various quantization formats.

**å®‰è£…æ­¥éª¤ | Installation:**

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# ç¼–è¯‘ï¼ˆCPU ç‰ˆæœ¬ï¼‰
make

# ç¼–è¯‘ï¼ˆCUDA GPU ç‰ˆæœ¬ï¼‰
make GGML_CUDA=1

# ç¼–è¯‘ï¼ˆMetal macOS ç‰ˆæœ¬ï¼‰
make GGML_METAL=1
```

**ä¸‹è½½ GLM-4.7-Flash æ¨¡å‹ | Download Model:**

```bash
# ä» HuggingFace ä¸‹è½½ GGUF æ ¼å¼æ¨¡å‹
# æ¨è: unsloth/GLM-4.7-Flash-GGUF

# åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p models
cd models

# ä¸‹è½½æ¨¡å‹ï¼ˆä½¿ç”¨ huggingface-cliï¼‰
pip install huggingface-hub
huggingface-cli download unsloth/GLM-4.7-Flash-GGUF --local-dir ./glm-4.7-flash
```

**å¯åŠ¨æœåŠ¡å™¨ | Start Server:**

```bash
# å¯åŠ¨ llama.cpp æœåŠ¡å™¨
./server \
  -m models/glm-4.7-flash/GLM-4.7-Flash-Q4_K_M.gguf \
  --host 0.0.0.0 \
  --port 8033 \
  -c 4096 \
  -n 2048

# å‚æ•°è¯´æ˜:
# -m: æ¨¡å‹è·¯å¾„
# --host: ç›‘å¬åœ°å€
# --port: ç›‘å¬ç«¯å£
# -c: ä¸Šä¸‹æ–‡é•¿åº¦
# -n: æœ€å¤§ç”Ÿæˆtokenæ•°
```

**é…ç½® MediCare AI | Configure MediCare AI:**

```bash
AI_API_KEY=your_api_key
AI_API_URL=http://localhost:8033/v1/
AI_MODEL_ID=unsloth/GLM-4.7-Flash-GGUF:BF16
```

**å‚è€ƒæ–‡æ¡£ | Reference:**
- GitHub: https://github.com/ggerganov/llama.cpp
- æ–‡æ¡£: https://github.com/ggerganov/llama.cpp/blob/master/docs

---

### 3. vLLMï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰| vLLM (Recommended for Production)

**ç®€ä»‹ | Introduction:**
vLLM æ˜¯ä¸€ä¸ªé«˜ååé‡ã€ä½å»¶è¿Ÿçš„ LLM æ¨ç†å’ŒæœåŠ¡å¼•æ“ï¼Œæ”¯æŒ PagedAttention æŠ€æœ¯ã€‚

vLLM is a high-throughput, low-latency LLM inference and serving engine with PagedAttention technology.

**å®‰è£…æ­¥éª¤ | Installation:**

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv vllm_env
source vllm_env/bin/activate

# å®‰è£… vLLM
pip install vllm

# æˆ–è€…ä½¿ç”¨ Docker
docker pull vllm/vllm-openai:latest
```

**å¯åŠ¨æœåŠ¡ | Start Service:**

```bash
# ä½¿ç”¨ Python
python -m vllm.entrypoints.openai.api_server \
  --model unsloth/glm-4.7b \
  --host 0.0.0.0 \
  --port 8000 \
  --tensor-parallel-size 1 \
  --max-num-seqs 256

# æˆ–è€…ä½¿ç”¨ Docker
docker run --runtime nvidia --gpus all \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model unsloth/glm-4.7b
```

**é…ç½® MediCare AI | Configure MediCare AI:**

```bash
AI_API_KEY=vllm
AI_API_URL=http://localhost:8000/v1/
AI_MODEL_ID=unsloth/glm-4.7b
```

**å‚è€ƒæ–‡æ¡£ | Reference:**
- GitHub: https://github.com/vllm-project/vllm
- æ–‡æ¡£: https://docs.vllm.ai

---

### 4. SGLangï¼ˆé«˜æ€§èƒ½æ¨ç†ï¼‰| SGLang (High-Performance Inference)

**ç®€ä»‹ | Introduction:**
SGLang æ˜¯ä¸€ä¸ªç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“æ„åŒ–ç”Ÿæˆè¯­è¨€ï¼Œæä¾›é«˜æ€§èƒ½æ¨ç†ã€‚

SGLang is a structured generation language for large language models, providing high-performance inference.

**å®‰è£…æ­¥éª¤ | Installation:**

```bash
pip install sglang

# å®‰è£…æ‰€æœ‰ä¾èµ–
pip install sglang[all]
```

**å¯åŠ¨æœåŠ¡ | Start Service:**

```bash
python -m sglang.launch_server \
  --model-path unsloth/glm-4.7b \
  --host 0.0.0.0 \
  --port 30000
```

**é…ç½® MediCare AI | Configure MediCare AI:**

```bash
AI_API_KEY=sglang
AI_API_URL=http://localhost:30000/v1/
AI_MODEL_ID=unsloth/glm-4.7b
```

**å‚è€ƒæ–‡æ¡£ | Reference:**
- GitHub: https://github.com/sgl-project/sglang
- æ–‡æ¡£: https://sglang.readthedocs.io

---

### 5. Text Generation Inference (HuggingFace) | TGI

**ç®€ä»‹ | Introduction:**
HuggingFace å¼€å‘çš„ç”¨äºéƒ¨ç½²å’ŒæœåŠ¡ LLM çš„ç”Ÿäº§å°±ç»ªå·¥å…·åŒ…ã€‚

A toolkit for deploying and serving LLMs developed by HuggingFace.

**ä½¿ç”¨ Docker éƒ¨ç½² | Deploy with Docker:**

```bash
docker run --gpus all --shm-size 1g -p 8080:80 \
  -v ~/.cache/huggingface:/data \
  ghcr.io/huggingface/text-generation-inference:2.0.0 \
  --model-id unsloth/glm-4.7b \
  --quantize eetq
```

**é…ç½® MediCare AI | Configure MediCare AI:**

```bash
AI_API_KEY=tgi
AI_API_URL=http://localhost:8080/v1/
AI_MODEL_ID=unsloth/glm-4.7b
```

**å‚è€ƒæ–‡æ¡£ | Reference:**
- GitHub: https://github.com/huggingface/text-generation-inference

---

## ğŸ“„ æœ¬åœ° MinerU æ–‡æ¡£å¤„ç†éƒ¨ç½² | Local MinerU Document Processing

### ç®€ä»‹ | Introduction

MinerU æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ–‡æ¡£å†…å®¹æå–å·¥å…·ï¼Œå¯ä»¥å°† PDFã€å›¾ç‰‡ç­‰æ ¼å¼çš„æ–‡æ¡£è½¬æ¢ä¸ºç»“æ„åŒ–çš„ Markdown æˆ– JSON æ ¼å¼ã€‚

MinerU is a powerful document content extraction tool that can convert documents in PDF, image, and other formats into structured Markdown or JSON formats.

### å®‰è£…æ­¥éª¤ | Installation

#### 1. ç¯å¢ƒè¦æ±‚ | Requirements

```bash
# Python 3.10+
python --version

# å®‰è£…ç³»ç»Ÿä¾èµ– (Ubuntu/Debian)
sudo apt-get update
sudo apt-get install -y \
  libgl1-mesa-glx \
  libglib2.0-0 \
  libsm6 \
  libxext6 \
  libxrender-dev \
  libgomp1
```

#### 2. å®‰è£… MinerU | Install MinerU

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n mineru python=3.10
conda activate mineru

# å®‰è£… Magic-PDFï¼ˆåŒ…å« MinerUï¼‰
pip install magic-pdf[full] --extra-index-url https://wheels.myhloli.com

# æˆ–è€…ä½¿ç”¨æºç å®‰è£…
git clone https://github.com/opendatalab/MinerU.git
cd MinerU
pip install -r requirements.txt
pip install -e .
```

#### 3. ä¸‹è½½æ¨¡å‹ | Download Models

```bash
# å®‰è£…æ¨¡å‹ä¸‹è½½å·¥å…·
pip install huggingface-hub

# ä¸‹è½½æ¨¡å‹æƒé‡
huggingface-cli download opendatalab/PDF-Extract-Kit-1.0 --local-dir ./models/PDF-Extract-Kit-1.0

# æˆ–è€…ä» ModelScope ä¸‹è½½
# pip install modelscope
# python scripts/download_models.py
```

#### 4. é…ç½®æ–‡ä»¶ | Configuration

åˆ›å»ºé…ç½®æ–‡ä»¶ `magic-pdf.json`ï¼š

```json
{
  "models_dir": "/path/to/models",
  "device_mode": "cuda",
  "table_config": {
    "model": "tablemaster",
    "enable": true
  }
}
```

#### 5. å¯åŠ¨æœåŠ¡ | Start Service

**å‘½ä»¤è¡Œä½¿ç”¨ | Command Line:**

```bash
# å¤„ç†å•ä¸ª PDF
magic-pdf pdf-document --input /path/to/document.pdf --output /path/to/output

# æ‰¹é‡å¤„ç†
magic-pdf pdf-directory --input /path/to/pdfs/ --output /path/to/output
```

**API æœåŠ¡ï¼ˆéœ€è¦è‡ªè¡Œå°è£…ï¼‰| API Service (needs wrapper):**

ç”±äº MinerU ä¸»è¦æä¾›å‘½ä»¤è¡Œå·¥å…·ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªç®€å•çš„ FastAPI åŒ…è£…å™¨ï¼š

```python
# mineru_service.py
from fastapi import FastAPI, File, UploadFile
import subprocess
import tempfile
import os

app = FastAPI()

@app.post("/extract")
async def extract_document(file: UploadFile = File(...)):
    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        content = await file.read()
        tmp.write(content)
        tmp_path = tmp.name
    
    # è°ƒç”¨ MinerU å¤„ç†
    output_dir = tempfile.mkdtemp()
    subprocess.run([
        "magic-pdf", "pdf-document",
        "--input", tmp_path,
        "--output", output_dir
    ])
    
    # è¯»å–ç»“æœ
    result_file = os.path.join(output_dir, "document.md")
    with open(result_file, 'r') as f:
        content = f.read()
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    os.unlink(tmp_path)
    
    return {"extracted_text": content}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
```

**Docker éƒ¨ç½² | Docker Deployment:**

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£… MinerU
RUN pip install magic-pdf[full] --extra-index-url https://wheels.myhloli.com

# å¤åˆ¶æ¨¡å‹ï¼ˆéœ€è¦æå‰ä¸‹è½½ï¼‰
COPY models /app/models

# é…ç½®æ–‡ä»¶
COPY magic-pdf.json /app/magic-pdf.json

# å¯åŠ¨è„šæœ¬
COPY start.sh /app/start.sh
RUN chmod +x /app/start.sh

EXPOSE 8001

CMD ["/app/start.sh"]
```

### é…ç½® MediCare AI | Configure MediCare AI

```bash
# MinerU API é…ç½®
MINERU_TOKEN=your_mineru_token_or_local_key
MINERU_API_URL=http://localhost:8001/extract
```

### å‚è€ƒæ–‡æ¡£ | Reference

- **GitHub**: https://github.com/opendatalab/MinerU
- **ä¸­æ–‡æ–‡æ¡£**: https://github.com/opendatalab/MinerU/blob/master/README_zh-CN.md
- **è‹±æ–‡æ–‡æ¡£**: https://github.com/opendatalab/MinerU/blob/master/README.md
- **åœ¨çº¿æ¼”ç¤º**: https://opendatalab.com/OpenSourceTools

---

## ğŸ”§ ç³»ç»Ÿé…ç½®ç¤ºä¾‹ | System Configuration Example

### å®Œæ•´çš„ .env é…ç½® | Complete .env Configuration

```bash
# ============================================
# AI å¤§æ¨¡å‹é…ç½® | AI Model Configuration
# æ ¹æ®æ‚¨çš„éƒ¨ç½²æ–¹å¼é€‰æ‹©å¯¹åº”çš„é…ç½®
# ============================================

# é€‰é¡¹ 1: Ollama
AI_API_KEY=ollama
AI_API_URL=http://localhost:11434/v1/
AI_MODEL_ID=unsloth/glm-4.7b

# é€‰é¡¹ 2: llama.cpp
# AI_API_KEY=llamacpp
# AI_API_URL=http://localhost:8033/v1/
# AI_MODEL_ID=unsloth/GLM-4.7-Flash-GGUF:BF16

# é€‰é¡¹ 3: vLLM
# AI_API_KEY=vllm
# AI_API_URL=http://localhost:8000/v1/
# AI_MODEL_ID=unsloth/glm-4.7b

# ============================================
# MinerU é…ç½® | MinerU Configuration
# ============================================

# å¦‚æœä½¿ç”¨å®˜æ–¹ APIï¼ˆä¸æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰
# MINERU_TOKEN=your_official_token

# å¦‚æœä½¿ç”¨æœ¬åœ°éƒ¨ç½²
MINERU_TOKEN=local_mineru_key
MINERU_API_URL=http://localhost:8001/extract

# ============================================
# æ•°æ®åº“é…ç½® | Database Configuration
# ============================================
POSTGRES_PASSWORD=your_secure_password
REDIS_PASSWORD=your_secure_password

# ============================================
# JWT é…ç½® | JWT Configuration
# ============================================
JWT_SECRET_KEY=your_random_secret_key_min_32_chars
```

---

## âœ… éªŒè¯éƒ¨ç½² | Verify Deployment

### æµ‹è¯• AI æœåŠ¡ | Test AI Service

```bash
# æµ‹è¯•å¥åº·æ£€æŸ¥
curl http://localhost:11434/api/tags

# æµ‹è¯•å¯¹è¯ï¼ˆOllamaï¼‰
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "unsloth/glm-4.7b",
    "prompt": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
    "stream": false
  }'

# æµ‹è¯• OpenAI å…¼å®¹æ¥å£
curl -X POST http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "unsloth/glm-4.7b",
    "messages": [{"role": "user", "content": "ä½ å¥½"}]
  }'
```

### æµ‹è¯• MinerU | Test MinerU

```bash
# ä¸‹è½½æµ‹è¯•æ–‡æ¡£
wget https://example.com/test-document.pdf -O test.pdf

# ä½¿ç”¨ MinerU å¤„ç†
magic-pdf pdf-document --input test.pdf --output ./output

# æ£€æŸ¥ç»“æœ
ls ./output/
cat ./output/document.md
```

---

## ğŸš€ å¿«é€Ÿå¯åŠ¨æ£€æŸ¥æ¸…å• | Quick Start Checklist

éƒ¨ç½² MediCare AI å‰ï¼Œè¯·ç¡®ä¿ï¼š

- [ ] **AI æ¨¡å‹æœåŠ¡å·²å¯åŠ¨å¹¶è¿è¡Œæ­£å¸¸**
  - [ ] å¯ä»¥é€šè¿‡ HTTP è®¿é—® API
  - [ ] æ¨¡å‹å·²æ­£ç¡®åŠ è½½
  - [ ] å“åº”æ—¶é—´å¯æ¥å—ï¼ˆ< 30ç§’ï¼‰

- [ ] **MinerU æœåŠ¡å·²é…ç½®**
  - [ ] å‘½ä»¤è¡Œå·¥å…·å®‰è£…æˆåŠŸ
  - [ ] æ¨¡å‹æ–‡ä»¶å·²ä¸‹è½½
  - [ ] å¯ä»¥æˆåŠŸæå– PDF æ–‡æœ¬

- [ ] **ç¯å¢ƒå˜é‡å·²æ­£ç¡®é…ç½®**
  - [ ] AI_API_URL æŒ‡å‘æ­£ç¡®çš„åœ°å€
  - [ ] MINERU_TOKEN å·²è®¾ç½®
  - [ ] æ•°æ®åº“å¯†ç å·²è®¾ç½®ï¼ˆå¦‚æœä½¿ç”¨è¿œç¨‹æ•°æ®åº“ï¼‰

- [ ] **Docker æœåŠ¡å·²å¯åŠ¨**
  - [ ] PostgreSQL å®¹å™¨è¿è¡Œä¸­
  - [ ] Redis å®¹å™¨è¿è¡Œä¸­ï¼ˆå¯é€‰ï¼‰
  - [ ] MediCare AI åç«¯å¯ä»¥è¿æ¥æ•°æ®åº“

---

## ğŸ“š æ›´å¤šèµ„æº | Additional Resources

### AI æ¨¡å‹èµ„æº | AI Model Resources
- **HuggingFace**: https://huggingface.co/models
- **ModelScope**: https://modelscope.cn
- **OpenXLab**: https://openxlab.org.cn

### æ¨¡å‹æ¨è | Model Recommendations
- **GLM-4.7-Flash**: é€‚åˆä¸­æ–‡åŒ»ç–—åœºæ™¯ï¼Œæ¨ç†é€Ÿåº¦å¿«
- **Qwen2.5-7B**: é˜¿é‡Œå·´å·´å¼€æºæ¨¡å‹ï¼Œä¸­æ–‡è¡¨ç°ä¼˜ç§€
- **Baichuan2-7B**: ç™¾å·æ™ºèƒ½å¼€æºæ¨¡å‹ï¼ŒåŒ»ç–—é¢†åŸŸè¡¨ç°è‰¯å¥½
- **Llama-3-8B**: Meta å¼€æºæ¨¡å‹ï¼Œè‹±æ–‡è¡¨ç°ä¼˜ç§€

### ç¡¬ä»¶è¦æ±‚ | Hardware Requirements

| æ¨¡å‹å¤§å° | æ˜¾å­˜è¦æ±‚ | æ¨è GPU | è¯´æ˜ |
|---------|---------|---------|------|
| 4B-7B | 8-12GB | RTX 3060/4060 | é€‚åˆä¸ªäººä½¿ç”¨ |
| 7B-13B | 16-24GB | RTX 3090/4090 | é«˜æ€§èƒ½æ¨ç† |
| 70B+ | å¤šå¡æˆ– A100 | ä¸“ä¸š GPU | ä¼ä¸šçº§éƒ¨ç½² |

---

## ğŸ†˜ æ•…éšœæ’é™¤ | Troubleshooting

### AI æœåŠ¡è¿æ¥å¤±è´¥ | AI Service Connection Failed

```bash
# æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
netstat -tlnp | grep <port>

# æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
sudo ufw allow <port>/tcp

# æµ‹è¯•æœ¬åœ°è¿æ¥
curl -v http://localhost:<port>/health
```

### MinerU å¤„ç†å¤±è´¥ | MinerU Processing Failed

```bash
# æ£€æŸ¥ä¾èµ–å®‰è£…
pip list | grep magic-pdf

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
ls -la ~/.cache/huggingface/

# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
magic-pdf pdf-document --input test.pdf --output ./output --debug
```

### å†…å­˜ä¸è¶³ | Out of Memory

```bash
# ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼ˆå‡å°æ˜¾å­˜å ç”¨ï¼‰
# Q4_K_M é‡åŒ–çº§åˆ«å¯ä»¥åœ¨ä¿æŒè´¨é‡çš„åŒæ—¶å‡å°‘ 75% çš„æ˜¾å­˜ä½¿ç”¨

# å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
# åœ¨ llama.cpp ä¸­ä½¿ç”¨ -c 2048 è€Œä¸æ˜¯é»˜è®¤çš„ 4096

# å¯ç”¨äº¤æ¢åˆ†åŒº
sudo swapon -a
```

---

**æ³¨æ„ | Note:**
æ‰€æœ‰ IP åœ°å€ã€API Key å’Œæ•æ„Ÿä¿¡æ¯éƒ½éœ€è¦æ ¹æ®æ‚¨çš„å®é™…ç¯å¢ƒè¿›è¡Œé…ç½®ã€‚è¯·å‹¿ä½¿ç”¨æ–‡æ¡£ä¸­çš„ç¤ºä¾‹å€¼ä½œä¸ºç”Ÿäº§ç¯å¢ƒé…ç½®ã€‚

All IP addresses, API keys, and sensitive information need to be configured according to your actual environment. Do not use example values from this document as production configurations.

---

**æœ€åæ›´æ–° | Last Updated:** 2025-02-01  
**ç‰ˆæœ¬ | Version:** 2.0.0 - æœ¬åœ°éƒ¨ç½²æŒ‡å—ç‰ˆ
